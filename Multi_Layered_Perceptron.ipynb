{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of numbers from MNIST images using multilayer perceptron softmax model\n",
    "<br>\n",
    "\n",
    "### Loss: Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the MNIST dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train is (60000, 28, 28)\n",
      "shape of y_train is (60000,)\n",
      "shape of X_test is (10000, 28, 28)\n",
      "shape of y_test is (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"shape of X_train is {X_train.shape}\")\n",
    "print(f\"shape of y_train is {Y_train.shape}\")\n",
    "print(f\"shape of X_test is {X_test.shape}\")\n",
    "print(f\"shape of y_test is {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing: Flattening array and normalizing\n",
    "X_train = X_train.reshape(60000,784)/255\n",
    "X_test = X_test.reshape(10000,784)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
       "       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117647, 0.36862745, 0.60392157,\n",
       "       0.66666667, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235294, 0.6745098 , 0.99215686, 0.94901961,\n",
       "       0.76470588, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215686, 0.93333333,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.98431373, 0.36470588,\n",
       "       0.32156863, 0.32156863, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882353, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.71372549,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31372549, 0.61176471, 0.41960784, 0.99215686, 0.99215686,\n",
       "       0.80392157, 0.04313725, 0.        , 0.16862745, 0.60392157,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509804,\n",
       "       0.99215686, 0.74509804, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313725, 0.74509804, 0.99215686,\n",
       "       0.2745098 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.1372549 , 0.94509804, 0.88235294, 0.62745098,\n",
       "       0.42352941, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764706, 0.94117647, 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
       "       0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.36470588,\n",
       "       0.98823529, 0.99215686, 0.73333333, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.97647059, 0.99215686,\n",
       "       0.97647059, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980392,\n",
       "       0.71764706, 0.99215686, 0.99215686, 0.81176471, 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.58039216, 0.89803922, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.71372549, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882353, 0.83529412, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.31764706,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058824, 0.85882353,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.76470588,\n",
       "       0.31372549, 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568627, 0.6745098 ,\n",
       "       0.88627451, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156863, 0.04313725, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333333, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137255, 0.52941176, 0.51764706, 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Target label preprocessing: One hot encoding of output\n",
    "with tf.compat.v1.Session() as ses:\n",
    "    Y_train = ses.run(tf.one_hot(Y_train, 10))\n",
    "    Y_test = ses.run(tf.one_hot(Y_test, 10))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters:\n",
    "\n",
    "1. Number of layers:2\n",
    "2. NUmber of Neurons: \n",
    "        Layer 1: 512\n",
    "        Layer 2: 264\n",
    "        output layer: 10 (softmax)\n",
    "3. Weights:\n",
    "        W1 = 784 x 512\n",
    "        W2 = 512 x 264\n",
    "        W3 = 264 x 10\n",
    "4. bias:\n",
    "        b1 = 512\n",
    "        b2 = 264\n",
    "        b3 = 10\n",
    "5. learning rate\n",
    "6. epoch\n",
    "7. batch_size\n",
    "8. batches_per_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the neural network architecture\n",
    "\n",
    "## NUmber of hidden layers = 2\n",
    "n_input = 784\n",
    "hidden1 = 512\n",
    "hidden2 = 264\n",
    "output = 10\n",
    "\n",
    "## Weights - Xavier Glorat normal distribution\n",
    "## Formula: sqrt(2/(n_input + hidden1))\n",
    "weight1 = tf.Variable(tf.random.normal([n_input, hidden1],stddev=0.039, mean=0))\n",
    "weight2 = tf.Variable(tf.random.normal([hidden1, hidden2],stddev=0.051, mean=0))\n",
    "weight3 = tf.Variable(tf.random.normal([hidden2, output],stddev=0.085, mean=0))\n",
    "\n",
    "## bias initialization\n",
    "\n",
    "bias1 = tf.Variable(0.1 * np.random.randn(hidden1).astype(np.float32))\n",
    "bias2 = tf.Variable(0.1 * np.random.randn(hidden2).astype(np.float32))\n",
    "bias3 = tf.Variable(0.1 * np.random.randn(output).astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters initialization\n",
    "\n",
    "epoch = 10\n",
    "batch_size = 100\n",
    "batches_per_epoch = int(X_train.shape[0]/batch_size)\n",
    "learning_rate = 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating placeholders for input and output\n",
    "x = tf.compat.v1.placeholder(tf.float32, [None, 784])\n",
    "y = tf.compat.v1.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multilayer perceptron: building the neural network architecture\n",
    "\n",
    "def mlp(inpt, w1, w2, w3, b1, b2, b3):\n",
    "    '''Returns the multi layered perceptron output from input and initalized weights and biases'''\n",
    "    \n",
    "    ## Layer 1: 784 x 512\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(inpt,w1),b1))\n",
    "    print(layer_1.get_shape(), w1.get_shape(), b1.get_shape())\n",
    "\n",
    "    ## Layer 2: 512 x 264\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1,w2),b2))\n",
    "    print(layer_2.get_shape(), w2.get_shape(), b2.get_shape())\n",
    "\n",
    "    ## Layer 3: 264 x 10\n",
    "    layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2,w3),b3))\n",
    "    print(layer_3.get_shape(), w3.get_shape(), b3.get_shape())\n",
    "    \n",
    "    return layer_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 512) (784, 512) (512,)\n",
      "(None, 264) (512, 264) (264,)\n",
      "(None, 10) (264, 10) (10,)\n"
     ]
    }
   ],
   "source": [
    "# Prediction of numbers from images\n",
    "yhat = mlp(x, weight1, weight2, weight3, bias1,bias2, bias3)\n",
    "\n",
    "# Loss function\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.math.log(yhat), axis =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 1: Gradient Descent \n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "# Optimizer 2: Adam\n",
    "\n",
    "adam_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create tensorflow session and run it\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    # these list collects the loss value of train and test data for every epoch\n",
    "    train_loss, test_loss = [], []\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        \n",
    "        # these lists collects the train and test loss value of every batch\n",
    "        avg_trn_cost, avg_tst_cost = [], []\n",
    "    \n",
    "        for batch in range(batches_per_epoch):\n",
    "            \n",
    "            x_data = X_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            y_data = Y_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            \n",
    "            sess.run(adam_optimizer, feed_dict = {x : x_data, y : y_data})\n",
    "            \n",
    "            c = sess.run(cost, feed_dict = {x : x_data, y : y_data})\n",
    "            avg_trn_cost.append(c)\n",
    "            c_tst = sess.run(cost, feed_dict = {x : X_test, y : Y_test})            \n",
    "            avg_tst_cost.append(c_tst)   \n",
    "\n",
    "        train_loss.append(np.mean(avg_trn_cost))\n",
    "        test_loss.append(np.mean(avg_tst_cost))\n",
    "        \n",
    "    correct_prediction = tf.math.equal(tf.math.argmax(y,1), tf.math.argmax(yhat,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Train accuracy\", accuracy.eval({x : X_train , y : Y_train}))\n",
    "    print(\"Test accuracy\", accuracy.eval({x : X_test , y : Y_test}))\n",
    "    \n",
    "    ## Visual evaluation of predicted numbers from images\n",
    "    fig, axes = plt.subplots(1,10, figsize = (8,4))\n",
    "    for img, ax in zip(X_test[:10], axes):\n",
    "        ax.set_title(np.argmax(sess.run(yhat, feed_dict={x: [img]})))\n",
    "        ax.imshow(img.reshape(28,28))\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single perceptron softmax model reached an accuracy of 91.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3df5yVZZ3/8debYfihEpAYKtCChaQCQkxqS6tDroJZamWuvwrbjIf7xWj1Gwn7aP1BXx9SlLrsoi4Rfd1cRUsjSr6hKeOvNAFBEJBAtGUGS0BBR0eF4fP949wD54xnZg4w95zDzPv5eJzHnPs693Wfz32J85n7vq77uhQRmJmZNdap2AGYmVlpcoIwM7O8nCDMzCwvJwgzM8vLCcLMzPLqXOwAWkufPn1i4MCBxQ7jgLz99tsceuihxQ6jZLg9crk99nJb5DqQ9li2bNnWiDgi32ftJkEMHDiQpUuXFjuMA1JVVUVlZWWxwygZbo9cbo+93Ba5DqQ9JP25qc98i8nMzPJygjAzs7ycIMzMLK920wdhZu3Xzp07qa6u5t133wWgZ8+erF27tshRlY5C2qNbt27079+f8vLygo/rBGFmJa+6upoePXowcOBAJPHWW2/Ro0ePYodVMlpqj4hg27ZtVFdXM2jQoIKP2+ETxPzlNcxYtI7N2+s4uld3Jo8dwnkj+xU7LDPL8u677+5JDrbvJHH44YezZcuWfarXoRPE/OU1TH1gFXU76wGo2V7H1AdWAThJmJUYJ4cDsz/t16E7qWcsWscZ9Y/xZJdJbOx6MU92mcQZ9Y8xY9G6YodmZlZ0HTpBVLz5MNPL59C/01Y6Cfp32sr08jlUvPlwsUMzsxKyfft2brvttv2q+7nPfY7t27cXvP/111/Pj370o/36rtbWoRPE1C6/4BC9n1N2iN5napdfFCkiMytFzSWI+vr6ZusuXLiQXr16pRFW6jp0gujL1n0qN7ODw/zlNYye/iiDpjzI6OmPMn95zQEdb8qUKbz00kuMGDGCyZMnU1VVxZgxY7j44osZNmwYAOeddx6jRo3ihBNOYPbs2XvqDhw4kK1bt/LKK69w3HHH8c1vfpMTTjiBM888k7q6uma/d8WKFZxyyikMHz6cL37xi7zxxhsAzJw5k+OPP57hw4dz4YUXAvDYY48xYsQIRowYwciRI3nrrbcO6JyhgycI9ey/T+VmVvoaBp/UbK8j2Dv45ECSxPTp0/nYxz7GihUrmDFjBgDPPvssN954I2vWrAFg7ty5LFu2jKVLlzJz5ky2bdv2geOsX7+eiRMnsnr1anr16sX999/f7Pd+7Wtf4wc/+AErV65k2LBh3HDDDXviWb58OStXruSOO+4A4Ec/+hGzZs1ixYoVPPHEE3Tv3n2/z7dBh04QnH4tlDdqxPLumXIzOyjNWLRuz8jEBnU761t98MlJJ52U80zBzJkzOfHEEznllFPYtGkT69ev/0CdQYMGMWLECABGjRrFK6+80uTxd+zYwfbt2znttNMAGD9+PI8//jgAw4cP55JLLuGuu+6ic+fMYNTRo0dz9dVXM3PmTLZv376n/EB07AQx/AL4wkzoOQBQ5ucXZmbKzeygtHl7/ts2TZXvr+zptauqqvj973/P008/zfPPP8/IkSP3PPWdrWvXrnvel5WVsWvXrv367gcffJCJEyeybNkyRo0axa5du5gyZQpz5syhrq6OU045hRdffHG/jp2tQz8HAWSSgROCWbtxdK/u1ORJBkf32v9bLj169Gj2nv6OHTvo3bs3hxxyCC+++CLPPPPMfn9Xg549e9K7d2+eeOIJ/u7v/o6f//znnHbaaezevZtNmzYxZswYPvOZz3D33XdTW1vL66+/zrBhwxg2bBhPP/00L774Ip/4xCcOKAYnCDNrVyaPHZLzACxA9/IyJo8dst/HPPzwwxk9ejRDhw7lrLPO4uyzz875fNy4cdxxxx0MHz6cIUOGcMopp+z3d2W78847ueKKK3jnnXc45phj+NnPfkZ9fT2XXnopO3bsICK46qqr6NWrF//yL//C4sWLKSsr4/jjj+ess8464O9XRLTCaRRfRUVFeMGg9sXtkasjt8fatWs57rjj9my3NPdQR5tCp9C5qRq3I4CkZRFRkW//VK8gJI0D/g0oA+ZExPQ8+1wAXA8E8HxEXJyU1wOrkt3+JyLOSTNWM2s/zhvZr10nhLaSWoKQVAbMAs4AqoElkhZExJqsfQYDU4HREfGGpI9kHaIuIkakFZ+ZmTUvzVFMJwEbImJjRLwPzAPObbTPN4FZEfEGQES8lmI8Zma2D9K8xdQP2JS1XQ2c3GifYwEkPUXmNtT1EfG75LNukpYCu4DpETG/8RdImgBMAOjbty9VVVWtegJtrba29qA/h9bk9sjVkdujZ8+eOaOI6uvrW+VJ4fai0PZ499139+nfUJoJIt/cso17xDsDg4FKoD/whKShEbEd+GhEbJZ0DPCopFUR8VLOwSJmA7Mh00l9sHfgdeROyHzcHrk6cnusXbs2pxPWCwblKrQ9unXrxsiRIws+bpq3mKqBAVnb/YHNefb5dUTsjIiXgXVkEgYRsTn5uRGoAgo/KzMzO2BpJoglwGBJgyR1AS4EFjTaZz4wBkBSHzK3nDZK6i2pa1b5aGANZmZFcCDTfQPceuutvPPOO3k/q6yspFSH6KeWICJiF3AlsAhYC9wXEaslTZPUMGR1EbBN0hpgMTA5IrYBxwFLJT2flE/PHv1kZtaW0kwQpSzVuZgiYmFEHBsRH4uIG5OyayNiQfI+IuLqiDg+IoZFxLyk/A/J9onJz5+mGaeZtTMr74NbhsL1vTI/V953QIdrPN03wIwZM/jUpz7F8OHDue666wB4++23OfvssznxxBMZOnQo9957LzNnzmTz5s2MGTOGMWPGNPs999xzD8OGDWPo0KFcc801QKYD+rLLLmPo0KEMGzaMW265Bcid8vuyyy47oPNriqfaMLP2ZeV98JtJsDOZj2nHpsw27Pe8a9OnT+eFF15gxYoVADz00EOsX7+eZ599lojgnHPO4fHHH2fLli0cffTRPPjgg5mv3rGDnj17cvPNN7N48WL69OnT5Hds3ryZa665hmXLltG7d2/OPPNM5s+fz4ABA6ipqeGFF14A2LM63fTp03n55Zfp2rUrmzZtavK4B6Jjz+ZqZu3PI9P2JocGO+sy5a3koYce4qGHHmLkyJF88pOf5MUXX2T9+vUMGzaM3//+91xzzTU88cQT9OzZs+BjLlmyhMrKSo444gg6d+7MJZdcwuOPP84xxxzDxo0b+da3vsXvfvc7PvShDwH5p/xubU4QZta+7Kjet/L9EBFMnTqVFStWsGLFCjZs2MA3vvENjj32WJYtW8awYcOYOnUq06YVnpSamhevd+/ePP/881RWVjJr1iwuv/xyIHfK71NPPXW/pw5vjhOEmbUvTa0IeQArRTae7nvs2LHMnTuX2tpaAGpqanjttdfYvHkzhxxyCJdeeinf+c53eO655/LWz+fkk0/mscceY+vWrdTX13PPPfdw2mmnsXXrVnbv3s2Xv/xlvv/97/Pcc8/lTPn9wx/+kB07duyJpTW5D8LM2pfTr83tg4ADXimy8XTfM2bMYO3atXz6058G4LDDDuOuu+5iw4YNTJ48mU6dOlFeXs7tt98OwIQJEzjrrLM46qijWLx4cd7vOOqoo7jpppsYM2YMEcHnPvc5zj33XJ5//nm+/vWvs3v3bgBuuummD0z5PXHiRHr16rXf59cUT/ddQjryk7L5uD1ydeT22Nfpvll5X6bPYUd15srh9Gvb9cJgB+V032ZmReGVIluF+yDMzCwvJwgzOyi0l9vhxbI/7ecEYWYlr1u3bmzbts1JYj9FBNu2baNbt277VM99EGZW8vr37091dTVbtmwBMusa7Osvu/askPbo1q0b/fvv21BfJwgzK3nl5eUMGjRoz3ZVVdU+rWvQ3qXVHr7FZGZmeTlBmJlZXk4QZmaWlxOEmZnl5QRhZmZ5pZogJI2TtE7SBklTmtjnAklrJK2WdHdW+XhJ65PX+DTjNDOzD0ptmKukMmAWcAZQDSyRtCB7bWlJg4GpwOiIeEPSR5LyDwPXARVAAMuSum+kFa+ZmeVK8wriJGBDRGyMiPeBecC5jfb5JjCr4Rd/RLyWlI8FHo6I15PPHgbGpRirmZk1kmaC6AdkL5RanZRlOxY4VtJTkp6RNG4f6pqZWYrSfJJaecoaT6TSGRgMVAL9gSckDS2wLpImABMA+vbtS1VV1QGEW3y1tbUH/Tm0JrdHLrfHXm6LXGm1R5oJohoYkLXdH9icZ59nImIn8LKkdWQSRjWZpJFdt6rxF0TEbGA2ZBYMOtgXU+nIC8Lk4/bI5fbYy22RK632SPMW0xJgsKRBkroAFwILGu0zHxgDIKkPmVtOG4FFwJmSekvqDZyZlJmZWRtJ7QoiInZJupLML/YyYG5ErJY0DVgaEQvYmwjWAPXA5IjYBiDp+2SSDMC0iHg9rVjNzOyDUp3NNSIWAgsblV2b9T6Aq5NX47pzgblpxmdmZk3zk9RmZpaXE4SZmeXlBGFmZnk5QZiZWV5OEGZmlpcThJmZ5eUEYWZmeTlBmJlZXk4QZmaWlxOEmZnl5QRhZmZ5OUGYmVleLSYISV+R1CN5/z1JD0j6ZPqhmZlZMRVyBfGvEfGWpM+QWSv6TuD2dMMyM7NiKyRB1Cc/zwZuj4hfA13SC8nMzEpBIQmiRtJ/AhcACyV1LbCemZkdxAr5RX8BmZXfxkXEduDDwORUozIzs6IrZEW5o4AHI+I9SZXAcOC/Uo3KzMyKrpAriPuBekkfB34KDALuLuTgksZJWidpg6QpeT6/TNIWSSuS1+VZn9VnlS8o8HzMzKyVFHIFsTsidkn6EnBrRPy7pOUtVZJUBswCzgCqgSWSFkTEmka73hsRV+Y5RF1EjCggPjMzS0EhVxA7JV0EfA34bVJWXkC9k4ANEbExIt4H5gHn7l+YZmbW1gq5gvg6cAVwY0S8LGkQcFcB9foBm7K2q4GT8+z3ZUmnAn8CroqIhjrdJC0FdgHTI2J+44qSJgATAPr27UtVVVUBYZWu2trag/4cWpPbI5fbYy+3Ra7U2iMiWnyRee5haPIqL7DOV4A5WdtfBf690T6HA12T91cAj2Z9dnTy8xjgFeBjzX3fqFGj4mC3ePHiYodQUtweudwee7ktch1IewBLo4nfq4VMtVEJrCfTn3Ab8KfkL/6WVAMDsrb7A5sbJadtEfFesvkTYFTWZ5uTnxuBKmBkAd9pZmatpJA+iB8DZ0bEaRFxKpnpNm4poN4SYLCkQZK6ABcCOaORJB2VtXkOsDYp7508kIekPsBooHHntpmZpaiQPojyiFjXsBERf5LUYid1ZEY+XUnmIbsyYG5ErJY0jcwlzQJgkqRzyPQzvA5cllQ/DvhPSbvJJLHp8cHRT2ZmlqJCEsRSST8Ffp5sXwIsK+TgEbEQWNio7Nqs91OBqXnq/QEYVsh3mJlZOgpJEP8ETAQmAQIeJ9MXYWZm7ViLCSLpRL45eZmZWQfRZIKQtAqIpj6PiOGpRGRmZiWhuSuIz7dZFGZmVnKaTBAR8ee2DMTMzEqLF/4xM7O8nCDMzCyvQqba+LwkJxIzsw6mkF/8FwLrJf1Q0nFpB2RmZqWhxQQREZeSmSjvJeBnkp6WNEFSj9SjMzOzoino1lFEvElm6dF5ZNao/iLwnKRvpRibmZkVUSF9EF+Q9CvgUTIryZ0UEWcBJwLfSTk+MzMrkkLmYvoKcEtEPJ5dGBHvSPrHdMIyM7NiK2Qupq9JOjKZljuAJRHxl+SzR9IO0MzMiqOQW0zfAJ4FvgScDzzjKwczs/avkFtM3wVGRsQ2AEmHA38A5qYZmJmZFVcho5iqgbeytt8CNqUTjpmZlYpCEkQN8EdJ10u6DngG2CDpaklXN1dR0jhJ6yRtkDQlz+eXSdoiaUXyujzrs/GS1iev8ft6YmZmdmAKucX0UvJq8OvkZ7MPykkqA2YBZ5C5ClkiaUGetaXvjYgrG9X9MHAdUEGmY3xZUveNAuI1M7NWUMgophsAkienIyJqCzz2ScCGiNiY1J8HnAs0ThD5jAUejojXk7oPA+OAewr8bjMzO0AtJghJQ4GfAx9OtrcCX4uI1S1U7UduX0U1cHKe/b4s6VTgT8BVEbGpibr98sQ2AZgA0LdvX6qqqlo6nZJWW1t70J9Da3J75HJ77OW2yJVWexRyi2k2cHVELAaQVAn8BPjbFuopT1njJUx/A9wTEe9JugK4E/hsgXWJiNlJfFRUVERlZWULIZW2qqoqDvZzaE1uj1xuj73cFrnSao9COqkPbUgOABFRBRxaQL1qYEDWdn9gc/YOEbEtIt5LNn8CjCq0rpmZpauQBLFR0r9KGpi8vge8XEC9JcBgSYMkdSEzbfiC7B0kHZW1eQ6wNnm/CDhTUm9JvYEzkzIzM2sjhdxi+kfgBuCBZPtx4OstVYqIXZKuJPOLvQyYGxGrJU0DlkbEAmBSMoXHLuB14LKk7uuSvk8myQBMa+iwNjOzttFsgkiGqv4iIv5+fw4eEQuBhY3Krs16PxWY2kTdufhpbTOzomn2FlNE1APvSOrZRvGYmVmJKOQW07vAquRZhLcbCiNiUmpRmZlZ0RWSIB5MXtk+MOTUzMzal0ISRK+I+LfsAknfTikeMzMrEYUMc803Ud5lrRyHmZmVmCavICRdBFwMDJKU/fxCD2Bb2oGZmVlxNXeL6Q/Aq0Af4MdZ5W8BK9MMyszMiq/JBBERfwb+DHy67cIxM7NSUcia1F9KFu3ZIelNSW9JerMtgjMzs+IpZBTTD4EvRMTaFvc0M7N2o5BRTH91cjAz63gKuYJYKuleYD7QMDU3EfFA01XMzOxgV0iC+BDwDpkptxsEe2d3NTOzdqiQNalbnNrbzMzan0JGMR0r6RFJLyTbw5NFg8zMrB0rpJP6J2TWbNgJEBEryawOZ2Zm7VghCeKQiHi2UdmuNIIxM7PSUUiC2CrpYyRTfEs6n8wUHC2SNE7SOkkbJE1pZr/zJYWkimR7oKQ6SSuS1x2FfJ+ZmbWeQkYxTQRmA5+QVAO8DFzSUqVkudJZwBlANbBE0oKIWNNovx7AJOCPjQ7xUkSMKCA+MzNLQYtXEBGxMVmT+gjgExHxmWSeppacBGxI6r8PzAPOzbPf98k8rf3uPsRtZmYpK+QKAoCIeLvlvXL0AzZlbVcDJ2fvIGkkMCAifivpO43qD5K0HHgT+F5EPNH4CyRNACYA9O3bl6qqqn0MsbTU1tYe9OfQmtweudwee7ktcqXVHgUniP2gPGV7liqV1Am4hfyLD70KfDQitkkaBcyXdEJE5EwSGBGzydz+oqKiIiorK1sp9OKoqqriYD+H1uT2yOX22MttkSut9iikk3p/VQMDsrb7A5uztnsAQ4EqSa8ApwALJFVExHsRsQ0gIpYBLwHHphirmZk1UsiDcl9JOpKR9D1JD0j6ZAHHXgIMljRIUhcyz07sWZkuInZERJ+IGBgRA4FngHMiYqmkI5JObiQdAwwGNu7z2ZmZ2X4r5AriXyPiLUmfAcYCdwK3t1QpInYBVwKLgLXAfRGxWtI0See0UP1UYKWk54FfAldExOsFxGpmZq2kkD6I+uTn2cDtEfFrSdcXcvCIWAgsbFR2bRP7Vma9vx+4v5DvMDOzdBRyBVEj6T+BC4CFkroWWM/MzA5ihfyiv4DMbaJxEbEd+DAwOdWozMys6Aq5xXQU8GBEvCepEhgO/FeqUZmZWdEVcgVxP1Av6ePAT4FBwN2pRmVmZkVXSILYnYxI+hJwa0RcReaqwszM2rFCEsROSRcBXwN+m5SVpxeSmZmVgkISxNeBTwM3RsTLkgYBd6UblpmZFVshs7muAb4DrJI0FKiOiOmpR2ZmZkXV4iimZOTSncArZCbgGyBpfEQ8nm5oZmZWTIUMc/0xcGZErAOQdCxwDzAqzcDMzKy4CumDKG9IDgAR8SfcSW1m1u4VcgWxTNJPgZ8n25cAy9ILyczMSkEhCeIKMutSTyLTB/E4cFuaQZmZWfE1e4spWfVtWUTcHBFfiogvRsQtEfFeG8XXIcxfXsPo6Y+yqmYHo6c/yvzlNcUOycys+QQREbuB5yV9tI3i6XDmL69h6gOrqNleB0DN9jqmPrDKScLMiq7QyfpWS3oWeLuhMCJaWvTHCjBj0TrqdtbnlNXtrGfGonWcN7JfkaIyMyssQdyQehQd2ObkyqHQcjOzttLkLSZJH5c0OiIey34BAVQXcnBJ4yStk7RB0pRm9jtfUkiqyCqbmtRbJ2nsvpzUweToXt33qdzMrK001wdxK/BWnvJ3ks+aJakMmAWcBRwPXCTp+Dz79SAzQuqPWWXHAxcCJwDjgNuS47U7k8cOoXt57ql1Ly9j8tghRYrIzCyjuQQxMCJWNi6MiKXAwAKOfRKwISI2RsT7wDzg3Dz7fR/4IfBuVtm5wLyIeC8iXgY2JMdrd84b2Y+bvjSMfskVQ79e3bnpS8Pc/2BmRddcgujWzGeF3P/oB2zK2q5OyvaQNBIYEBG/JVeLdduT88qe4qmukxjW6WWe6jqJ88qeKnZIZmbNdlIvkfTNiPhJdqGkb1DYk9TKUxZZx+kE3AJctq91s44xAZgA0LdvX6qqqgoIq8TUvQE7/gJHXk5t16OpOvJyWPMXqPkVdO9d7OiKqra29uD8b5oSt8debotcabVHcwnin4FfScqeWqMC6AJ8sYBjVwMDsrb7A5uztnsAQ4EqSQBHAgsknVNAXQAiYjYwG6CioiIqKysLCKvE3DIUdmQulqqG3EDluusy5T0HwFUvFDGw4quqquKg/G+aErfHXm6LXGm1R5MJIiL+CvytpDFkfpEDPBgRjxZ47CXA4GSBoRoync4XZx1/B9CnYVtSFfCdiFgqqQ64W9LNwNHAYODZgs/qYLKjiQFhTZWbmbWRFp+DiIjFwOJ9PXBE7JJ0JbAIKAPmRsRqSdOApRGxoJm6qyXdB6wBdgETI6K+qf0Paj3777mC+EC5mVkRFfKg3H6LiIXAwkZl1zaxb2Wj7RuBG1MLrlScfi38ZhLszHowrrx7ptzMrIhSTRBWgOEXZH4+Mi3zs+eATHJoKDczKxIniFIw/ILMq6oKLurYHdNmVjoKWVHOzMw6ICcIMzPLywnCzMzycoIwM7O8nCDMzCwvJwgzM8vLCcLMzPLycxC2x/zlNcxYtI7N2+s4uld3Jo8d4nUpzDowJwgDMslh6gOrqNuZmfKqZnsdUx9YBeAkYdZB+RaTATBj0bo9yaFB3c56ZixaV6SIzKzYnCAMgM3b6/ap3MzaPycIA+DoXvlXkW2q3MzaPycIA2Dy2CF0Ly/LKeteXsbksUOKFJGZFZs7qQ3IdET32/RbBjw3g4/EFl7TEWz65GQ+NXJcsUMzsyJxgrCMlffxqVXXAXUgOJItHLnqOhjY22tTmHVQvsVkGY9My13VDjLbDQsZmVmHk2qCkDRO0jpJGyRNyfP5FZJWSVoh6UlJxyflAyXVJeUrJN2RZpwG7Kjet3Iza/dSu8UkqQyYBZwBVANLJC2IiDVZu90dEXck+58D3Aw03PR+KSJGpBWfNdKzP+zYlL/czDqkNK8gTgI2RMTGiHgfmAecm71DRLyZtXkoECnGY805/VoobzSktbx7pryNzV9ew+jpj7KqZgejpz/K/OU1bR6DmYEi0vmdLOl8YFxEXJ5sfxU4OSKubLTfROBqoAvw2YhYL2kgsBr4E/Am8L2IeCLPd0wAJgD07dt31Lx581I5l7ZSW1vLYYcdVrwA6t6At16F+vehrAv0OAq6927TELbX7aTmjTp2R9C3O/y1DjpJ9OvdnV7dy9s0llJT9H8fJcRtketA2mPMmDHLIqIi32dpjmJSnrIPZKOImAXMknQx8D1gPPAq8NGI2CZpFDBf0gmNrjiIiNnAbICKioqorKxs5VNoW1VVVRzs53CgRk9/lJrtmecx/vewXfx4VeafaL9eZTw1pbKIkRWf/33s5bbIlVZ7pHmLqRoYkLXdH9jczP7zgPMAIuK9iNiWvF8GvAQcm1KcVkI85YdZ6UgzQSwBBksaJKkLcCGwIHsHSYOzNs8G1iflRySd3Eg6BhgMbEwxVisRnvLDrHSkliAiYhdwJbAIWAvcFxGrJU1LRiwBXClptaQVZPohxiflpwIrJT0P/BK4IiJeTytWKx2Txw7h/C5/4Mkukximl3myyyTO7/IHT/lhVgSpPkkdEQuBhY3Krs16/+0m6t0P3J9mbFaazit7is+Xz6Fz/btsEPTvtJXpZXPoXHYi4Ce6zdqSp9qw0vLINDrXv5tT1Ln+3cwT3W085YdX2LOOzgnCSkuJPNHtFfbMPBeTlZqmntxu4ye6vcKemROElZoSeaLbw23NnCCs1Ay/AL4wE3omj9D0HJDZbuP+Bw+3NXMfhJWi4RdkXlVVcNELRQlh8tghPPmr2/hn5nG0trI5+nArF/KZsf+rKPGYFYOvIMzyOK/sKaaXz6F/p610ahhuWz6H88qeavNYPHmhFYsThFk+zQ23bUMNo6lqkr6PhtFUThLWFpwgzPIpkeG2Hk1lxeQEYZZPiQy33by9jnM6PZkz9cg5nZ70aCprE+6kNsvn9GvhN5Ny1+kuwnDb8Yc9y3d3zuEQvb936pHyOXy4vAuZ+S3bjp8s73h8BWGWT85wWxVtuO13y+/lEL2fU3aI3ue75fe2aRzZfSGB+0I6Cl9BmDWlYbhtER1S95d9Kk9Lc30hvopov3wFYVbKSrAvZGPXi4vWF+Ihv23LCcKslJXI1CPjD3s273Mh4w97ts1i8JDftucEYVbKSmTqkVLoC5mxaB1n1D+WM6LrjPrHijLkt+FKZtCUB9v1lYz7IMxKXQlMPVIKfSEVbz7MTeUfHNE19U2Az7ZZHB1pKvhUryAkjZO0TtIGSVPyfH6FpFWSVkh6UtLxWZ9NTeqtkzQ2zTjNrAUl0Bcytcsv8l7FTO3yizaLAXKvZBr6Y4p9JZNWn0xqCUJSGTALOAs4HrgoOwEk7o6IYRExAvghcHNS93jgQruP5fIAAAcASURBVOAEYBxwW3I8MyuGEugL6cvWfSpPS8WbD+ftj6l48+E2jaMt+mTSvII4CdgQERsj4n1gHnBu9g4R8WbW5qFAJO/PBeZFxHsR8TKwITmemRVDCTwXoiauVpoqT0spXsmk1SejiGh5r/05sHQ+MC4iLk+2vwqcHBFXNtpvInA10AX4bESsl/QfwDMRcVeyz0+B/xcRv2xUdwIwAaBv376j5s2bl8q5tJXa2loOO+ywYodRMtweuTp8e9S9ATs2QeymtuvRHPbeZlCnTLLq3rvt4nh1RdOfHTWizcLYVFNDP22lk2JPe+wOURN9GNCv8L6QMWPGLIuIinyfpdlJrTxlH8hGETELmCXpYuB7wPh9qDsbmA1QUVERlZWVBxJv0VVVVXGwn0NrcnvkcnsAK++DR6ZRdeTlVP5lTuYW1/Avtm0Mt1yZSVSN9RzQpoMI/nL9xzmSLQBUDbmBynXXZco5giMv2dAq35HmLaZqYEDWdn9gczP7zwPO28+6ZtYRDL8Arnoh85f6VS8U50n3EuiPgbbpk0kzQSwBBksaJKkLmU7nBdk7SBqctXk2sD55vwC4UFJXSYOAwUDbPZFjZtaUEuiPgbbpk0ntFlNE7JJ0JbAIKAPmRsRqSdOApRGxALhS0t8DO4E3yNxeItnvPmANsAuYGBH1eb/IzKytlcA8XW0x43CqD8pFxEJgYaOya7Pef7uZujcCN6YXnZnZQawhQTWscthzQNIn03qJy09Sm5kdrFJ+yt5zMZmZWV5OEGZmlpcThJmZ5eUEYWZmeTlBmJlZXqnNxdTWJG0B/lzsOA5QH2jjqSlLm9sjl9tjL7dFrgNpj7+JiCPyfdBuEkR7IGlpU5NmdURuj1xuj73cFrnSag/fYjIzs7ycIMzMLC8niNIyu9gBlBi3Ry63x15ui1yptIf7IMzMLC9fQZiZWV5OEGZmlpcTRAmQNEDSYklrJa2W1OQ06B2FpDJJyyX9ttixFJukXpJ+KenF5N/Ip4sdUzFJuir5/+QFSfdI6lbsmNqSpLmSXpP0QlbZhyU9LGl98rNVFul2gigNu4D/HRHHAacAEyUdX+SYiu3bwNpiB1Ei/g34XUR8AjiRDtwukvoBk4CKiBhKZjGyC4sbVZv7v8C4RmVTgEciYjDwSLJ9wJwgSkBEvBoRzyXv3yLzC6BfcaMqHkn9ySxBO6fYsRSbpA8BpwI/BYiI9yNie3GjKrrOQHdJnYFD6GDr1UfE48DrjYrPBe5M3t8JnNca3+UEUWIkDQRGAn8sbiRFdSvwXWB3sQMpAccAW4CfJbfc5kg6tNhBFUtE1AA/Av4HeBXYEREPFTeqktA3Il6FzB+cwEda46BOECVE0mHA/cA/R8SbxY6nGCR9HngtIpYVO5YS0Rn4JHB7RIwE3qaVbh8cjJJ76+cCg4CjgUMlXVrcqNovJ4gSIamcTHL474h4oNjxFNFo4BxJrwDzgM9Kuqu4IRVVNVAdEQ1XlL8kkzA6qr8HXo6ILRGxE3gA+Nsix1QK/irpKIDk52utcVAniBIgSWTuMa+NiJuLHU8xRcTUiOgfEQPJdD4+GhEd9i/EiPgLsEnSkKTodGBNEUMqtv8BTpF0SPL/zel04E77LAuA8cn78cCvW+OgnVvjIHbARgNfBVZJWpGU/UtELCxiTFY6vgX8t6QuwEbg60WOp2gi4o+Sfgk8R2b033I62LQbku4BKoE+kqqB64DpwH2SvkEmiX6lVb7LU22YmVk+vsVkZmZ5OUGYmVleThBmZpaXE4SZmeXlBGFmZnk5QZi1QFK9pBVZr1Z7klnSwOxZOc1KiZ+DMGtZXUSMKHYQZm3NVxBm+0nSK5J+IOnZ5PXxpPxvJD0iaWXy86NJeV9Jv5L0fPJqmCKiTNJPkjUOHpLUPdl/kqQ1yXHmFek0rQNzgjBrWfdGt5j+IeuzNyPiJOA/yMxCS/L+vyJiOPDfwMykfCbwWEScSGY+pdVJ+WBgVkScAGwHvpyUTwFGJse5Iq2TM2uKn6Q2a4Gk2og4LE/5K8BnI2JjMtniXyLicElbgaMiYmdS/mpE9JG0BegfEe9lHWMg8HCy0AuSrgHKI+L/SPodUAvMB+ZHRG3Kp2qWw1cQZgcmmnjf1D75vJf1vp69fYNnA7OAUcCyZIEcszbjBGF2YP4h6+fTyfs/sHcZzEuAJ5P3jwD/BHvW3P5QUweV1AkYEBGLySye1Av4wFWMWZr8F4lZy7pnzbILmfWhG4a6dpX0RzJ/bF2UlE0C5kqaTGY1uIbZV78NzE5m3KwnkyxebeI7y4C7JPUEBNzipUatrbkPwmw/JX0QFRGxtdixmKXBt5jMzCwvX0GYmVlevoIwM7O8nCDMzCwvJwgzM8vLCcLMzPJygjAzs7z+P289Y6g3+m+MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plotting cross entropy loss as a function of epoch\n",
    "plt.scatter(range(1, epoch+1), train_loss, label = \"train loss\")\n",
    "plt.scatter(range(1, epoch+1), test_loss, label = \"test loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "_ = plt.xlabel(\"Epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
